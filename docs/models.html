<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>src.models API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.models</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
from torch import nn
import torch.nn.functional as F
import torch


class VideoNet(nn.Module):
    def __init__(self):

        super(VideoNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=1)
        self.conv2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3)
        self.lstm = nn.LSTM(input_size=1, hidden_size=40, num_layers=1, batch_first=True)
        self.fc = nn.Linear(40, 1) 
#         self.conv2 = nn.Conv1d(in_channels=H, out_channels=3, kernel_size=5)
#         self.maxpool2 = nn.MaxPool1d(kernel_size=2)
#         self.fc = nn.Linear(18 + p, 1) # this is hard-coded
    
    def forward(self, x):
        &#39;&#39;&#39;
        x: torch.Tensor
            (batch_size, time_steps, height, width)
          = (batch_size, 40, 10, 10)
        &#39;&#39;&#39;
#         print(&#39;in shape&#39;, x.shape)
        # extract features from each time_step separately
        # reshape time_steps and batch into same dim
        batch_size = x.shape[0]
        T = x.shape[1]
        x = x.reshape(batch_size * T, 1, x.shape[2], x.shape[3])
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = torch.max(x, dim=3).values
        x = torch.max(x, dim=2).values
        
        # extract time_steps back out
        # run lstm on result 1D time series
        x = x.reshape(batch_size, T, 1)
        outputs, (h1, c1) = self.lstm(x) # get hidden vec
        h1 = h1.squeeze(0) # remove dimension corresponding to multiple layers / directions
        return self.fc(h1)

class FCNN(nn.Module):
    
    &#34;&#34;&#34;
    customized (one hidden layer) fully connected neural network class
    &#34;&#34;&#34;

    def __init__(self, D_in, H, p):
        
        &#34;&#34;&#34;
        Parameters:        
        ==========================================================
            D_in: int
                dimension of input track
                
            H: int
                hidden layer size
                
            p: int
                number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)            
        &#34;&#34;&#34;

        super(FCNN, self).__init__()
        self.fc1 = nn.Linear(D_in, H)
        #self.fc2 = nn.Linear(H, H)
        self.bn1 = nn.BatchNorm1d(H)
        self.fc2 = nn.Linear(H + p, 1) 
    
    def forward(self, x1, x2):
        
        z1 = self.fc1(x1)
        z1 = self.bn1(z1)
        h1 = F.relu(z1)
        if x2 is not None:
            h1 = torch.cat((h1, x2), 1)
        z2 = self.fc2(h1)
        #h2 = F.relu(z2)
        #z3 = self.fc3(h2)       
        
        return z2
    
    
class LSTMNet(nn.Module):
    def __init__(self, D_in, H, p):
        
        &#34;&#34;&#34;
        Parameters:        
        ==========================================================
            D_in: int
                dimension of input track (ignored, can be variable)
                
            H: int
                hidden layer size
                
            p: int
                number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)            
        &#34;&#34;&#34;

        super(LSTMNet, self).__init__()
        self.lstm = nn.LSTM(input_size=1, hidden_size=H, num_layers=1, batch_first=True)
        self.fc = nn.Linear(H + p, 1) 
    
    def forward(self, x1, x2=None):
        x1 = x1.unsqueeze(2) # add input_size dimension (this is usually for the size of embedding vector)
        outputs, (h1, c1) = self.lstm(x1) # get hidden vec
        h1 = h1.squeeze(0) # remove dimension corresponding to multiple layers / directions
        if x2 is not None:
            h1 = torch.cat((h1, x2), 1)
        return self.fc(h1)
    
class CNN(nn.Module):
    def __init__(self, D_in, H, p):
        
        &#34;&#34;&#34;
        Parameters:        
        ==========================================================
            D_in: int
                dimension of input track (ignored, can be variable)
                
            H: int
                hidden layer size
                
            p: int
                number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)            
        &#34;&#34;&#34;

        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=H, kernel_size=7)
        self.maxpool1 = nn.MaxPool1d(kernel_size=2)
        self.conv2 = nn.Conv1d(in_channels=H, out_channels=3, kernel_size=5)
        self.maxpool2 = nn.MaxPool1d(kernel_size=2)
        self.fc = nn.Linear(18 + p, 1) # this is hard-coded
    
    def forward(self, x1, x2):
        x1 = x1.unsqueeze(1) # add channel dim
        x1 = self.conv1(x1)
        x1 = self.maxpool1(x1)
        x1 = self.conv2(x1)
        x1 = self.maxpool2(x1)
        x1 = x1.reshape(x1.shape[0], -1) # flatten channel dim
        
        if x2 is not None:
            x1 = torch.cat((x1, x2), 1)
        return self.fc(x1)
    
class AttentionNet(nn.Module):
    
    &#34;&#34;&#34;
    customized (one hidden layer) fully connected neural network class
    &#34;&#34;&#34;

    def __init__(self, D_in, H, p):
        
        &#34;&#34;&#34;
        Parameters:        
        ==========================================================
            D_in: int
                dimension of input track (ignored, can be variable)
                
            H: int
                hidden layer size
                
            p: int
                number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)            
        &#34;&#34;&#34;

        super(AttentionNet, self).__init__()
        self.att1 = nn.MultiheadAttention(embed_dim=18, num_heads=3)
        self.ln1 = nn.LayerNorm(D_in)
        self.fc1 = nn.Linear(D_in, 1) 
        self.relu1 = nn.ReLU()
        self.att2 = nn.MultiheadAttention(embed_dim=18, num_heads=3)
        self.ln2 = nn.LayerNorm(D_in)
        self.fc2 = nn.Linear(D_in + p, 1) 
    
    def forward(self, x1, x2):
        print(x1.shape)
        x1 = self.att1(x1, x1)
        x1 = self.ln1(x1)
        x1 = self.fc1(x1)
        x1 = self.relu1(x1)
        x1 = self.att2(x1, x1)
        x1 = self.ln2(x1)
        
        if x2 is not None:
            h1 = torch.cat((h1, x2), 1)
        return self.fc2(h1)

class MaxLinear(nn.Module):
    &#39;&#39;&#39;Takes flattened input and predicts it using many linear units
        X: batch_size x num_timepoints
    &#39;&#39;&#39;

    def __init__(self, input_dim=24300, num_units=20, nonlin=F.relu, use_bias=False):
        super(MaxLinear, self).__init__()

        self.fc1 = nn.Linear(input_dim, num_units, bias=use_bias)

    #         self.offset = nn.Parameter(torch.Tensor([0]))

    def forward(self, X, **kwargs):
        #         print(&#39;in shape&#39;, X.shape, X.dtype)
        X = self.fc1(X)  # .max(dim=-1)
        #         print(&#39;out shape&#39;, X.shape, X.dtype)
        X = torch.max(X, dim=1)[0]  # 0 because this returns max, indices
        #         print(&#39;out2 shape&#39;, X.shape, X.dtype)
        return X  # + self.offset


class MaxConv(nn.Module):
    &#39;&#39;&#39;Takes flattened input and predicts it using many conv unit
        X: batch_size x 1 x num_timepoints
            OR
        X: list of size (num_timepoints,)
    &#39;&#39;&#39;

    def __init__(self, num_units=20, kernel_size=30, nonlin=F.relu, use_bias=False):
        super(MaxConv, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_units, kernel_size=kernel_size, bias=use_bias)
        #         torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=&#39;zeros&#39;)
        self.offset = nn.Parameter(torch.Tensor([0]))

    def forward(self, X, **kwargs):
        if type(X) == list:
            print(&#39;list&#39;)
            X = torch.tensor(np.array(X).astype(np.float32))
            X = X.unsqueeze(0)
            X = X.unsqueeze(0)
            print(X.shape)
        #         print(&#39;in shape&#39;, X.shape, X.dtype)
        else:
            X = X.unsqueeze(1)
        X = self.conv1(X)  # .max(dim=-1)
        #         print(&#39;out shape&#39;, X.shape, X.dtype)
        # max over channels
        X = torch.max(X, dim=1)[0]  # 0 because this returns max, indices

        # max over time step
        X = torch.max(X, dim=1)[0] + self.offset  # 0 because this returns max, indices
        #         print(&#39;out2 shape&#39;, X.shape, X.dtype)

        X = X.unsqueeze(1)

        #         print(&#39;preds&#39;, X)
        return X


class MaxConvLinear(nn.Module):
    &#39;&#39;&#39;Takes input patch, uses linear filter to convert it to time series, then runs temporal conv, then takes max
        X: batch_size x H_patch x W_patch x time
    &#39;&#39;&#39;

    def __init__(self, num_timepoints=300, num_linear_filts=1, num_conv_filts=3, patch_size=9,
                 kernel_size=30, nonlin=F.relu, use_bias=False):
        super(MaxConvLinear, self).__init__()
        self.fc1 = nn.Linear(patch_size * patch_size, num_linear_filts, bias=use_bias)
        self.conv1 = nn.Conv1d(in_channels=num_linear_filts, out_channels=num_conv_filts, kernel_size=kernel_size,
                               bias=use_bias)
        self.offset = nn.Parameter(torch.Tensor([0]))

    def forward(self, X, **kwargs):
        s = X.shape  # batch_size x H_patch x W_patch x time
        X = X.reshape(s[0], s[1] * s[2], s[3])
        X = torch.transpose(X, 1, 2)
        #         print(&#39;in shape&#39;, X.shape, X.dtype)
        X = self.fc1(X)  # .max(dim=-1)
        X = torch.transpose(X, 1, 2)

        X = self.conv1(X)  # .max(dim=-1)
        #         print(&#39;out shape&#39;, X.shape, X.dtype)
        # max over channels
        X = torch.max(X, dim=1)[0]  # 0 because this returns max, indices

        # max over time step
        X = torch.max(X, dim=1)[0]  # + self.offset # 0 because this returns max, indices
        #         print(&#39;out2 shape&#39;, X.shape, X.dtype)

        X = X.unsqueeze(1)
        return X</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.models.AttentionNet"><code class="flex name class">
<span>class <span class="ident">AttentionNet</span></span>
<span>(</span><span>D_in, H, p)</span>
</code></dt>
<dd>
<section class="desc"><p>customized (one hidden layer) fully connected neural network class</p>
<h1 id="parameters">Parameters:</h1>
<pre><code>D_in: int
    dimension of input track (ignored, can be variable)

H: int
    hidden layer size

p: int
    number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AttentionNet(nn.Module):
    
    &#34;&#34;&#34;
    customized (one hidden layer) fully connected neural network class
    &#34;&#34;&#34;

    def __init__(self, D_in, H, p):
        
        &#34;&#34;&#34;
        Parameters:        
        ==========================================================
            D_in: int
                dimension of input track (ignored, can be variable)
                
            H: int
                hidden layer size
                
            p: int
                number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)            
        &#34;&#34;&#34;

        super(AttentionNet, self).__init__()
        self.att1 = nn.MultiheadAttention(embed_dim=18, num_heads=3)
        self.ln1 = nn.LayerNorm(D_in)
        self.fc1 = nn.Linear(D_in, 1) 
        self.relu1 = nn.ReLU()
        self.att2 = nn.MultiheadAttention(embed_dim=18, num_heads=3)
        self.ln2 = nn.LayerNorm(D_in)
        self.fc2 = nn.Linear(D_in + p, 1) 
    
    def forward(self, x1, x2):
        print(x1.shape)
        x1 = self.att1(x1, x1)
        x1 = self.ln1(x1)
        x1 = self.fc1(x1)
        x1 = self.relu1(x1)
        x1 = self.att2(x1, x1)
        x1 = self.ln2(x1)
        
        if x2 is not None:
            h1 = torch.cat((h1, x2), 1)
        return self.fc2(h1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.AttentionNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x1, x2)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x1, x2):
    print(x1.shape)
    x1 = self.att1(x1, x1)
    x1 = self.ln1(x1)
    x1 = self.fc1(x1)
    x1 = self.relu1(x1)
    x1 = self.att2(x1, x1)
    x1 = self.ln2(x1)
    
    if x2 is not None:
        h1 = torch.cat((h1, x2), 1)
    return self.fc2(h1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.models.CNN"><code class="flex name class">
<span>class <span class="ident">CNN</span></span>
<span>(</span><span>D_in, H, p)</span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<h1 id="parameters">Parameters:</h1>
<pre><code>D_in: int
    dimension of input track (ignored, can be variable)

H: int
    hidden layer size

p: int
    number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CNN(nn.Module):
    def __init__(self, D_in, H, p):
        
        &#34;&#34;&#34;
        Parameters:        
        ==========================================================
            D_in: int
                dimension of input track (ignored, can be variable)
                
            H: int
                hidden layer size
                
            p: int
                number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)            
        &#34;&#34;&#34;

        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=H, kernel_size=7)
        self.maxpool1 = nn.MaxPool1d(kernel_size=2)
        self.conv2 = nn.Conv1d(in_channels=H, out_channels=3, kernel_size=5)
        self.maxpool2 = nn.MaxPool1d(kernel_size=2)
        self.fc = nn.Linear(18 + p, 1) # this is hard-coded
    
    def forward(self, x1, x2):
        x1 = x1.unsqueeze(1) # add channel dim
        x1 = self.conv1(x1)
        x1 = self.maxpool1(x1)
        x1 = self.conv2(x1)
        x1 = self.maxpool2(x1)
        x1 = x1.reshape(x1.shape[0], -1) # flatten channel dim
        
        if x2 is not None:
            x1 = torch.cat((x1, x2), 1)
        return self.fc(x1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.CNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x1, x2)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x1, x2):
    x1 = x1.unsqueeze(1) # add channel dim
    x1 = self.conv1(x1)
    x1 = self.maxpool1(x1)
    x1 = self.conv2(x1)
    x1 = self.maxpool2(x1)
    x1 = x1.reshape(x1.shape[0], -1) # flatten channel dim
    
    if x2 is not None:
        x1 = torch.cat((x1, x2), 1)
    return self.fc(x1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.models.FCNN"><code class="flex name class">
<span>class <span class="ident">FCNN</span></span>
<span>(</span><span>D_in, H, p)</span>
</code></dt>
<dd>
<section class="desc"><p>customized (one hidden layer) fully connected neural network class</p>
<h1 id="parameters">Parameters:</h1>
<pre><code>D_in: int
    dimension of input track

H: int
    hidden layer size

p: int
    number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FCNN(nn.Module):
    
    &#34;&#34;&#34;
    customized (one hidden layer) fully connected neural network class
    &#34;&#34;&#34;

    def __init__(self, D_in, H, p):
        
        &#34;&#34;&#34;
        Parameters:        
        ==========================================================
            D_in: int
                dimension of input track
                
            H: int
                hidden layer size
                
            p: int
                number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)            
        &#34;&#34;&#34;

        super(FCNN, self).__init__()
        self.fc1 = nn.Linear(D_in, H)
        #self.fc2 = nn.Linear(H, H)
        self.bn1 = nn.BatchNorm1d(H)
        self.fc2 = nn.Linear(H + p, 1) 
    
    def forward(self, x1, x2):
        
        z1 = self.fc1(x1)
        z1 = self.bn1(z1)
        h1 = F.relu(z1)
        if x2 is not None:
            h1 = torch.cat((h1, x2), 1)
        z2 = self.fc2(h1)
        #h2 = F.relu(z2)
        #z3 = self.fc3(h2)       
        
        return z2</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.FCNN.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x1, x2)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x1, x2):
    
    z1 = self.fc1(x1)
    z1 = self.bn1(z1)
    h1 = F.relu(z1)
    if x2 is not None:
        h1 = torch.cat((h1, x2), 1)
    z2 = self.fc2(h1)
    #h2 = F.relu(z2)
    #z3 = self.fc3(h2)       
    
    return z2</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.models.LSTMNet"><code class="flex name class">
<span>class <span class="ident">LSTMNet</span></span>
<span>(</span><span>D_in, H, p)</span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<h1 id="parameters">Parameters:</h1>
<pre><code>D_in: int
    dimension of input track (ignored, can be variable)

H: int
    hidden layer size

p: int
    number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)
</code></pre></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LSTMNet(nn.Module):
    def __init__(self, D_in, H, p):
        
        &#34;&#34;&#34;
        Parameters:        
        ==========================================================
            D_in: int
                dimension of input track (ignored, can be variable)
                
            H: int
                hidden layer size
                
            p: int
                number of additional covariates (such as lifetime, msd, etc..., to be concatenated to the hidden layer)            
        &#34;&#34;&#34;

        super(LSTMNet, self).__init__()
        self.lstm = nn.LSTM(input_size=1, hidden_size=H, num_layers=1, batch_first=True)
        self.fc = nn.Linear(H + p, 1) 
    
    def forward(self, x1, x2=None):
        x1 = x1.unsqueeze(2) # add input_size dimension (this is usually for the size of embedding vector)
        outputs, (h1, c1) = self.lstm(x1) # get hidden vec
        h1 = h1.squeeze(0) # remove dimension corresponding to multiple layers / directions
        if x2 is not None:
            h1 = torch.cat((h1, x2), 1)
        return self.fc(h1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.LSTMNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x1, x2=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x1, x2=None):
    x1 = x1.unsqueeze(2) # add input_size dimension (this is usually for the size of embedding vector)
    outputs, (h1, c1) = self.lstm(x1) # get hidden vec
    h1 = h1.squeeze(0) # remove dimension corresponding to multiple layers / directions
    if x2 is not None:
        h1 = torch.cat((h1, x2), 1)
    return self.fc(h1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.models.MaxConv"><code class="flex name class">
<span>class <span class="ident">MaxConv</span></span>
<span>(</span><span>num_units=20, kernel_size=30, nonlin=&lt;function relu&gt;, use_bias=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes flattened input and predicts it using many conv unit
X: batch_size x 1 x num_timepoints
OR
X: list of size (num_timepoints,)</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaxConv(nn.Module):
    &#39;&#39;&#39;Takes flattened input and predicts it using many conv unit
        X: batch_size x 1 x num_timepoints
            OR
        X: list of size (num_timepoints,)
    &#39;&#39;&#39;

    def __init__(self, num_units=20, kernel_size=30, nonlin=F.relu, use_bias=False):
        super(MaxConv, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_units, kernel_size=kernel_size, bias=use_bias)
        #         torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=&#39;zeros&#39;)
        self.offset = nn.Parameter(torch.Tensor([0]))

    def forward(self, X, **kwargs):
        if type(X) == list:
            print(&#39;list&#39;)
            X = torch.tensor(np.array(X).astype(np.float32))
            X = X.unsqueeze(0)
            X = X.unsqueeze(0)
            print(X.shape)
        #         print(&#39;in shape&#39;, X.shape, X.dtype)
        else:
            X = X.unsqueeze(1)
        X = self.conv1(X)  # .max(dim=-1)
        #         print(&#39;out shape&#39;, X.shape, X.dtype)
        # max over channels
        X = torch.max(X, dim=1)[0]  # 0 because this returns max, indices

        # max over time step
        X = torch.max(X, dim=1)[0] + self.offset  # 0 because this returns max, indices
        #         print(&#39;out2 shape&#39;, X.shape, X.dtype)

        X = X.unsqueeze(1)

        #         print(&#39;preds&#39;, X)
        return X</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.MaxConv.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X, **kwargs):
    if type(X) == list:
        print(&#39;list&#39;)
        X = torch.tensor(np.array(X).astype(np.float32))
        X = X.unsqueeze(0)
        X = X.unsqueeze(0)
        print(X.shape)
    #         print(&#39;in shape&#39;, X.shape, X.dtype)
    else:
        X = X.unsqueeze(1)
    X = self.conv1(X)  # .max(dim=-1)
    #         print(&#39;out shape&#39;, X.shape, X.dtype)
    # max over channels
    X = torch.max(X, dim=1)[0]  # 0 because this returns max, indices

    # max over time step
    X = torch.max(X, dim=1)[0] + self.offset  # 0 because this returns max, indices
    #         print(&#39;out2 shape&#39;, X.shape, X.dtype)

    X = X.unsqueeze(1)

    #         print(&#39;preds&#39;, X)
    return X</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.models.MaxConvLinear"><code class="flex name class">
<span>class <span class="ident">MaxConvLinear</span></span>
<span>(</span><span>num_timepoints=300, num_linear_filts=1, num_conv_filts=3, patch_size=9, kernel_size=30, nonlin=&lt;function relu&gt;, use_bias=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes input patch, uses linear filter to convert it to time series, then runs temporal conv, then takes max
X: batch_size x H_patch x W_patch x time</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaxConvLinear(nn.Module):
    &#39;&#39;&#39;Takes input patch, uses linear filter to convert it to time series, then runs temporal conv, then takes max
        X: batch_size x H_patch x W_patch x time
    &#39;&#39;&#39;

    def __init__(self, num_timepoints=300, num_linear_filts=1, num_conv_filts=3, patch_size=9,
                 kernel_size=30, nonlin=F.relu, use_bias=False):
        super(MaxConvLinear, self).__init__()
        self.fc1 = nn.Linear(patch_size * patch_size, num_linear_filts, bias=use_bias)
        self.conv1 = nn.Conv1d(in_channels=num_linear_filts, out_channels=num_conv_filts, kernel_size=kernel_size,
                               bias=use_bias)
        self.offset = nn.Parameter(torch.Tensor([0]))

    def forward(self, X, **kwargs):
        s = X.shape  # batch_size x H_patch x W_patch x time
        X = X.reshape(s[0], s[1] * s[2], s[3])
        X = torch.transpose(X, 1, 2)
        #         print(&#39;in shape&#39;, X.shape, X.dtype)
        X = self.fc1(X)  # .max(dim=-1)
        X = torch.transpose(X, 1, 2)

        X = self.conv1(X)  # .max(dim=-1)
        #         print(&#39;out shape&#39;, X.shape, X.dtype)
        # max over channels
        X = torch.max(X, dim=1)[0]  # 0 because this returns max, indices

        # max over time step
        X = torch.max(X, dim=1)[0]  # + self.offset # 0 because this returns max, indices
        #         print(&#39;out2 shape&#39;, X.shape, X.dtype)

        X = X.unsqueeze(1)
        return X</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.MaxConvLinear.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X, **kwargs):
    s = X.shape  # batch_size x H_patch x W_patch x time
    X = X.reshape(s[0], s[1] * s[2], s[3])
    X = torch.transpose(X, 1, 2)
    #         print(&#39;in shape&#39;, X.shape, X.dtype)
    X = self.fc1(X)  # .max(dim=-1)
    X = torch.transpose(X, 1, 2)

    X = self.conv1(X)  # .max(dim=-1)
    #         print(&#39;out shape&#39;, X.shape, X.dtype)
    # max over channels
    X = torch.max(X, dim=1)[0]  # 0 because this returns max, indices

    # max over time step
    X = torch.max(X, dim=1)[0]  # + self.offset # 0 because this returns max, indices
    #         print(&#39;out2 shape&#39;, X.shape, X.dtype)

    X = X.unsqueeze(1)
    return X</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.models.MaxLinear"><code class="flex name class">
<span>class <span class="ident">MaxLinear</span></span>
<span>(</span><span>input_dim=24300, num_units=20, nonlin=&lt;function relu&gt;, use_bias=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Takes flattened input and predicts it using many linear units
X: batch_size x num_timepoints</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaxLinear(nn.Module):
    &#39;&#39;&#39;Takes flattened input and predicts it using many linear units
        X: batch_size x num_timepoints
    &#39;&#39;&#39;

    def __init__(self, input_dim=24300, num_units=20, nonlin=F.relu, use_bias=False):
        super(MaxLinear, self).__init__()

        self.fc1 = nn.Linear(input_dim, num_units, bias=use_bias)

    #         self.offset = nn.Parameter(torch.Tensor([0]))

    def forward(self, X, **kwargs):
        #         print(&#39;in shape&#39;, X.shape, X.dtype)
        X = self.fc1(X)  # .max(dim=-1)
        #         print(&#39;out shape&#39;, X.shape, X.dtype)
        X = torch.max(X, dim=1)[0]  # 0 because this returns max, indices
        #         print(&#39;out2 shape&#39;, X.shape, X.dtype)
        return X  # + self.offset</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.MaxLinear.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X, **kwargs):
    #         print(&#39;in shape&#39;, X.shape, X.dtype)
    X = self.fc1(X)  # .max(dim=-1)
    #         print(&#39;out shape&#39;, X.shape, X.dtype)
    X = torch.max(X, dim=1)[0]  # 0 because this returns max, indices
    #         print(&#39;out2 shape&#39;, X.shape, X.dtype)
    return X  # + self.offset</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.models.VideoNet"><code class="flex name class">
<span>class <span class="ident">VideoNet</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VideoNet(nn.Module):
    def __init__(self):

        super(VideoNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=1)
        self.conv2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=3)
        self.lstm = nn.LSTM(input_size=1, hidden_size=40, num_layers=1, batch_first=True)
        self.fc = nn.Linear(40, 1) 
#         self.conv2 = nn.Conv1d(in_channels=H, out_channels=3, kernel_size=5)
#         self.maxpool2 = nn.MaxPool1d(kernel_size=2)
#         self.fc = nn.Linear(18 + p, 1) # this is hard-coded
    
    def forward(self, x):
        &#39;&#39;&#39;
        x: torch.Tensor
            (batch_size, time_steps, height, width)
          = (batch_size, 40, 10, 10)
        &#39;&#39;&#39;
#         print(&#39;in shape&#39;, x.shape)
        # extract features from each time_step separately
        # reshape time_steps and batch into same dim
        batch_size = x.shape[0]
        T = x.shape[1]
        x = x.reshape(batch_size * T, 1, x.shape[2], x.shape[3])
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = torch.max(x, dim=3).values
        x = torch.max(x, dim=2).values
        
        # extract time_steps back out
        # run lstm on result 1D time series
        x = x.reshape(batch_size, T, 1)
        outputs, (h1, c1) = self.lstm(x) # get hidden vec
        h1 = h1.squeeze(0) # remove dimension corresponding to multiple layers / directions
        return self.fc(h1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.VideoNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"><p>x: torch.Tensor
(batch_size, time_steps, height, width)
= (batch_size, 40, 10, 10)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def forward(self, x):
        &#39;&#39;&#39;
        x: torch.Tensor
            (batch_size, time_steps, height, width)
          = (batch_size, 40, 10, 10)
        &#39;&#39;&#39;
#         print(&#39;in shape&#39;, x.shape)
        # extract features from each time_step separately
        # reshape time_steps and batch into same dim
        batch_size = x.shape[0]
        T = x.shape[1]
        x = x.reshape(batch_size * T, 1, x.shape[2], x.shape[3])
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = torch.max(x, dim=3).values
        x = torch.max(x, dim=2).values
        
        # extract time_steps back out
        # run lstm on result 1D time series
        x = x.reshape(batch_size, T, 1)
        outputs, (h1, c1) = self.lstm(x) # get hidden vec
        h1 = h1.squeeze(0) # remove dimension corresponding to multiple layers / directions
        return self.fc(h1)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.models.AttentionNet" href="#src.models.AttentionNet">AttentionNet</a></code></h4>
<ul class="">
<li><code><a title="src.models.AttentionNet.forward" href="#src.models.AttentionNet.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.CNN" href="#src.models.CNN">CNN</a></code></h4>
<ul class="">
<li><code><a title="src.models.CNN.forward" href="#src.models.CNN.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.FCNN" href="#src.models.FCNN">FCNN</a></code></h4>
<ul class="">
<li><code><a title="src.models.FCNN.forward" href="#src.models.FCNN.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.LSTMNet" href="#src.models.LSTMNet">LSTMNet</a></code></h4>
<ul class="">
<li><code><a title="src.models.LSTMNet.forward" href="#src.models.LSTMNet.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.MaxConv" href="#src.models.MaxConv">MaxConv</a></code></h4>
<ul class="">
<li><code><a title="src.models.MaxConv.forward" href="#src.models.MaxConv.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.MaxConvLinear" href="#src.models.MaxConvLinear">MaxConvLinear</a></code></h4>
<ul class="">
<li><code><a title="src.models.MaxConvLinear.forward" href="#src.models.MaxConvLinear.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.MaxLinear" href="#src.models.MaxLinear">MaxLinear</a></code></h4>
<ul class="">
<li><code><a title="src.models.MaxLinear.forward" href="#src.models.MaxLinear.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.VideoNet" href="#src.models.VideoNet">VideoNet</a></code></h4>
<ul class="">
<li><code><a title="src.models.VideoNet.forward" href="#src.models.VideoNet.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>