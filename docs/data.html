<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>src.data API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.data</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import sys
from copy import deepcopy
from os.path import join as oj

import mat4py
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
try:
    from skimage.external.tifffile import imread
except:
    from skimage.io import imread

pd.options.mode.chained_assignment = None  # default=&#39;warn&#39; - caution: this turns off setting with copy warning
import pickle as pkl
from viz import *
import math
import config
import features
import outcomes
import load_tracking
from tqdm import tqdm
import train_reg

def load_dfs_for_lstm(dsets=[&#39;clath_aux+gak_new&#39;],
                      splits=[&#39;test&#39;],
                      meta=[&#39;cell_num&#39;, &#39;Y_sig_mean&#39;, &#39;Y_sig_mean_normalized&#39;],
                      length=40,
                      normalize=True,
                      filter_short=True,
                      padding=&#39;end&#39;):
    &#39;&#39;&#39;Loads dataframes preprocessed ready for LSTM
    &#39;&#39;&#39;
    dfs = {}
    for dset in tqdm(dsets):
        for split in splits:
            df = get_data(dset=dset)
            if filter_short:
                df = df[~(df.short | df.long | df.hotspots)]
        #         df = df[df.valid]
                df = df[df.lifetime &gt; 15] # only keep hard tracks
            else:
                df = df[~df.hotspots]
            df = df[df.cell_num.isin(config.DSETS[dset][split])] # exclude held-out test data
            feat_names = [&#39;X_same_length_normalized&#39;] + select_final_feats(get_feature_names(df))

            # downsample tracks
            df[&#39;X_same_length&#39;] = [features.downsample(df.iloc[i][&#39;X&#39;], length, padding=padding)
                                   for i in range(len(df))] # downsampling
            # normalize tracks
            df = features.normalize_track(df, track=&#39;X_same_length&#39;, by_time_point=False)

            # regression response
            df = train_reg.add_sig_mean(df, resp_tracks=[&#39;Y&#39;])     

            # remove extraneous feats
            # df = df[feat_names + meta]
    #         df = df.dropna() 

            # normalize features
            if normalize:
                for feat in feat_names:
                    if &#39;X_same_length&#39; not in feat:
                        df = features.normalize_feature(df, feat)

            dfs[(dset, split)] = deepcopy(df)
    return dfs, feat_names
    

def get_data(dset=&#39;clath_aux+gak_a7d2&#39;, use_processed=True, save_processed=True,
             processed_file=oj(config.DIR_PROCESSED, &#39;df.pkl&#39;),
             metadata_file=oj(config.DIR_PROCESSED, &#39;metadata.pkl&#39;),
             use_processed_dicts=True,
             compute_dictionary_learning=False,
             outcome_def=&#39;y_consec_thresh&#39;,
             pixel_data: bool=False,
             video_data: bool=True,
             acc_thresh=0.95,
             previous_meta_file: str=None):
    &#39;&#39;&#39;
    Params
    ------
    use_processed: bool, optional
        determines whether to load df from cached pkl
    save_processed: bool, optional
        if not using processed, determines whether to save the df
    use_processed_dicts: bool, optional
        if False, recalculate the dictionary features
    previous_meta_file: str, optional
        filename for metadata.pkl file saved by previous preprocessing
        the thresholds for lifetime are taken from this file
    &#39;&#39;&#39;
    # get things based onn dset
    DSET = config.DSETS[dset]
    LABELS = config.LABELS[dset]

    processed_file = processed_file[:-4] + &#39;_&#39; + dset + &#39;.pkl&#39;
    metadata_file = metadata_file[:-4] + &#39;_&#39; + dset + &#39;.pkl&#39;

    if use_processed and os.path.exists(processed_file):
        return pd.read_pickle(processed_file)
    else:
        print(&#39;loading + preprocessing data...&#39;)
        metadata = {}
        
        
        # load tracks
        print(&#39;\tloading tracks...&#39;)
        df = load_tracking.get_tracks(data_dir=DSET[&#39;data_dir&#39;],
                                      split=DSET, 
                                      pixel_data=pixel_data, 
                                      video_data=video_data,
                                      dset=dset)  # note: different Xs can be different shapes
#         df = df.fillna(df.median()) # this only does anything for the dynamin tracks, where x_pos is sometimes NaN
#         print(&#39;num nans&#39;, df.isna().sum())
        df[&#39;pid&#39;] = np.arange(df.shape[0])  # assign each track a unique id
        df[&#39;valid&#39;] = True  # all tracks start as valid
        
        # set testing tracks to not valid
        if DSET[&#39;test&#39;] is not None:
            df[&#39;valid&#39;][df.cell_num.isin(DSET[&#39;test&#39;])] = False
        metadata[&#39;num_tracks&#39;] = df.valid.sum()
        # print(&#39;training&#39;, df.valid.sum())

        
        
        # preprocess data
        print(&#39;\tpreprocessing data...&#39;)
        df = remove_invalid_tracks(df)  # use catIdx
        # print(&#39;valid&#39;, df.valid.sum())
        df = features.add_basic_features(df)
        df = outcomes.add_outcomes(df, LABELS=LABELS)

        metadata[&#39;num_tracks_valid&#39;] = df.valid.sum()
        metadata[&#39;num_aux_pos_valid&#39;] = df[df.valid][outcome_def].sum()
        metadata[&#39;num_hotspots_valid&#39;] = df[df.valid][&#39;hotspots&#39;].sum()
        df[&#39;valid&#39;][df.hotspots] = False
        df, meta_lifetime = process_tracks_by_lifetime(df, outcome_def=outcome_def,
                                                       plot=False, acc_thresh=acc_thresh,
                                                       previous_meta_file=previous_meta_file)
        df[&#39;valid&#39;][df.short] = False
        df[&#39;valid&#39;][df.long] = False
        metadata.update(meta_lifetime)
        metadata[&#39;num_tracks_hard&#39;] = df[&#39;valid&#39;].sum()
        metadata[&#39;num_aux_pos_hard&#39;] = int(df[df.valid == 1][outcome_def].sum())

        
        # add features
        print(&#39;\tadding features...&#39;)
        df = features.add_dasc_features(df)
        if compute_dictionary_learning:
            df = features.add_dict_features(df, use_processed=use_processed_dicts)
        # df = features.add_smoothed_tracks(df)
        # df = features.add_pcs(df)
        # df = features.add_trend_filtering(df) 
        # df = features.add_binary_features(df, outcome_def=outcome_def)
        if save_processed:
            print(&#39;\tsaving...&#39;)
            pkl.dump(metadata, open(metadata_file, &#39;wb&#39;))
            df.to_pickle(processed_file)
    return df


def remove_invalid_tracks(df, keep=[1, 2]):
    &#39;&#39;&#39;Remove certain types of tracks based on cat_idx.
    Only keep cat_idx  = 1 and 2
    1-4 (non-complex trajectory - no merges and splits)
        1 - valid
        2 - signal occasionally drops out
        3 - cut  - starts / ends
        4 - multiple - at the same place (continues throughout)
    5-8 (there is merging or splitting)
    &#39;&#39;&#39;
    return df[df.catIdx.isin(keep)]


def process_tracks_by_lifetime(df: pd.DataFrame, outcome_def: str,
                               plot=False, acc_thresh=0.95, previous_meta_file=None):
    &#39;&#39;&#39;Calculate accuracy you can get by just predicting max class 
    as a func of lifetime and return points within proper lifetime (only looks at training cells)
    &#39;&#39;&#39;
    vals = df[df.valid == 1][[&#39;lifetime&#39;, outcome_def]]

    R, C = 1, 3
    lifetimes = np.unique(vals[&#39;lifetime&#39;])

    # cumulative accuracy for different thresholds
    accs_cum_lower = np.array([1 - np.mean(vals[outcome_def][vals[&#39;lifetime&#39;] &lt;= l]) for l in lifetimes])
    accs_cum_higher = np.array([np.mean(vals[outcome_def][vals[&#39;lifetime&#39;] &gt;= l]) for l in lifetimes]).flatten()

    if previous_meta_file is None:
        try:
            idx_thresh = np.nonzero(accs_cum_lower &gt;= acc_thresh)[0][-1]  # last nonzero index
            thresh_lower = lifetimes[idx_thresh]
        except:
            idx_thresh = 0
            thresh_lower = lifetimes[idx_thresh] - 1
        try:
            idx_thresh_2 = np.nonzero(accs_cum_higher &gt;= acc_thresh)[0][0]
            thresh_higher = lifetimes[idx_thresh_2]
        except:
            idx_thresh_2 = lifetimes.size - 1
            thresh_higher = lifetimes[idx_thresh_2] + 1
    else:
        previous_meta = pkl.load(open(previous_meta_file, &#39;rb&#39;))
        thresh_lower = previous_meta[&#39;thresh_short&#39;]
        thresh_higher = previous_meta[&#39;thresh_long&#39;]

    # only df with lifetimes in proper range
    df[&#39;short&#39;] = df[&#39;lifetime&#39;] &lt;= thresh_lower
    df[&#39;long&#39;] = df[&#39;lifetime&#39;] &gt;= thresh_higher
    n = vals.shape[0]
    n_short = np.sum(df[&#39;short&#39;])
    n_long = np.sum(df[&#39;long&#39;])
    acc_short = 1 - np.mean(vals[outcome_def][vals[&#39;lifetime&#39;] &lt;= thresh_lower])
    acc_long = np.mean(vals[outcome_def][vals[&#39;lifetime&#39;] &gt;= thresh_higher])

    metadata = {&#39;num_short&#39;: n_short, &#39;num_long&#39;: n_long, &#39;acc_short&#39;: acc_short,
                &#39;acc_long&#39;: acc_long, &#39;thresh_short&#39;: thresh_lower, &#39;thresh_long&#39;: thresh_higher}

    if plot:
        plt.figure(figsize=(12, 4), dpi=200)
        plt.subplot(R, C, 1)
        outcome = df[outcome_def]
        plt.hist(df[&#39;lifetime&#39;][outcome == 1], label=&#39;aux+&#39;, alpha=1, color=cb, bins=25)
        plt.hist(df[&#39;lifetime&#39;][outcome == 0], label=&#39;aux-&#39;, alpha=0.7, color=cr, bins=25)
        plt.xlabel(&#39;lifetime&#39;)
        plt.ylabel(&#39;count&#39;)
        plt.legend()

        plt.subplot(R, C, 2)
        plt.plot(lifetimes, accs_cum_lower, color=cr)
        #     plt.axvline(thresh_lower)
        plt.axvspan(0, thresh_lower, alpha=0.2, color=cr)
        plt.ylabel(&#39;fraction of negative events&#39;)
        plt.xlabel(f&#39;lifetime &lt;= value\nshaded includes {n_short / n * 100:0.0f}% of pts&#39;)

        plt.subplot(R, C, 3)
        plt.plot(lifetimes, accs_cum_higher, cb)
        plt.axvspan(thresh_higher, max(lifetimes), alpha=0.2, color=cb)
        plt.ylabel(&#39;fraction of positive events&#39;)
        plt.xlabel(f&#39;lifetime &gt;= value\nshaded includes {n_long / n * 100:0.0f}% of pts&#39;)
        plt.tight_layout()

    return df, metadata


def get_feature_names(df):
    &#39;&#39;&#39;Returns features (all of which are scalar)
    Removes metadata + time-series columns + outcomes
    &#39;&#39;&#39;
    ks = list(df.keys())
    feat_names = [
        k for k in ks
        if not k.startswith(&#39;y&#39;)
           and not k.startswith(&#39;Y&#39;)
           and not k.startswith(&#39;Z&#39;)
           and not k.startswith(&#39;pixel&#39;)
           #         and not k.startswith(&#39;pc_&#39;)
           and not k in [&#39;catIdx&#39;, &#39;cell_num&#39;, &#39;pid&#39;, &#39;valid&#39;,  # metadata
                         &#39;X&#39;, &#39;X_pvals&#39;, &#39;x_pos&#39;, &#39;X_starts&#39;, &#39;X_ends&#39;, &#39;X_extended&#39;,  # curves
                         &#39;short&#39;, &#39;long&#39;, &#39;hotspots&#39;, &#39;sig_idxs&#39;,  # should be weeded out
                         &#39;X_max_around_Y_peak&#39;, &#39;X_max_after_Y_peak&#39;,  # redudant with X_max / fall
                         &#39;X_max_diff&#39;, &#39;X_peak_idx&#39;,  # unlikely to be useful
                         &#39;t&#39;, &#39;x_pos_seq&#39;, &#39;y_pos_seq&#39;,  # curves
                         &#39;X_smooth_spl&#39;, &#39;X_smooth_spl_dx&#39;, &#39;X_smooth_spl_d2x&#39;,  # curves
                         &#39;X_quantiles&#39;,
                         ]
    ]
    return feat_names


def select_final_feats(feat_names, binarize=False):
    feat_names = [x for x in feat_names
                  if not x.startswith(&#39;sc_&#39;)  # sparse coding
                  and not x.startswith(&#39;nmf_&#39;) # nmf
                  and not x in [&#39;center_max&#39;, &#39;left_max&#39;, &#39;right_max&#39;, &#39;up_max&#39;, &#39;down_max&#39;,
                                &#39;X_max_around_Y_peak&#39;, &#39;X_max_after_Y_peak&#39;, &#39;X_max_diff_after_Y_peak&#39;]
                  and not x.startswith(&#39;pc_&#39;)
                  and not &#39;extended&#39; in x
                  and not x == &#39;slope_end&#39;
                  and not &#39;_tf_smooth&#39; in x
                  and not &#39;local&#39; in x
                  and not &#39;last&#39; in x
                  and not &#39;video&#39; in x
                  and not x == &#39;X_quantiles&#39;
                  #               and not &#39;X_peak&#39; in x
                  #               and not &#39;slope&#39; in x
                  #               and not x in [&#39;fall_final&#39;, &#39;fall_slope&#39;, &#39;fall_imp&#39;, &#39;fall&#39;]
                  ]

    if binarize:
        feat_names = [x for x in feat_names if &#39;binary&#39; in x]
    else:
        feat_names = [x for x in feat_names if not &#39;binary&#39; in x]
    return feat_names


if __name__ == &#39;__main__&#39;:
    
    # process original data (and save out lifetime thresholds)
    dset_orig = &#39;clath_aux+gak_a7d2&#39;
    df = get_data(dset=dset_orig)  # save out orig
    
    # process new data (using lifetime thresholds from original data)
    outcome_def = &#39;y_consec_sig&#39;
#     for dset in [&#39;clath_aux_dynamin&#39;]:
    for dset in config.DSETS.keys():
        df = get_data(dset=dset, previous_meta_file=None)
        # df = get_data(dset=dset, previous_meta_file=f&#39;{config.DIR_PROCESSED}/metadata_{dset_orig}.pkl&#39;)
        print(dset, &#39;num cells&#39;, len(df[&#39;cell_num&#39;].unique()), &#39;num tracks&#39;, df.shape[0], &#39;num aux+&#39;,
              df[outcome_def].sum(), &#39;aux+ fraction&#39;, (df[outcome_def].sum() / df.shape[0]).round(3),
              &#39;valid&#39;, df.valid.sum(), &#39;valid aux+&#39;, df[df.valid][outcome_def].sum(), &#39;valid aux+ fraction&#39;,
              (df[df.valid][outcome_def].sum() / df.valid.sum()).round(3))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.data.get_data"><code class="name flex">
<span>def <span class="ident">get_data</span></span>(<span>dset='clath_aux+gak_a7d2', use_processed=True, save_processed=True, processed_file='/accounts/projects/vision/chandan/auxilin-prediction/src/../data/processed/df.pkl', metadata_file='/accounts/projects/vision/chandan/auxilin-prediction/src/../data/processed/metadata.pkl', use_processed_dicts=True, compute_dictionary_learning=False, outcome_def='y_consec_thresh', pixel_data=False, video_data=True, acc_thresh=0.95, previous_meta_file=None)</span>
</code></dt>
<dd>
<section class="desc"><h2 id="params">Params</h2>
<dl>
<dt><strong><code>use_processed</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>determines whether to load df from cached pkl</dd>
<dt><strong><code>save_processed</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>if not using processed, determines whether to save the df</dd>
<dt><strong><code>use_processed_dicts</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>if False, recalculate the dictionary features</dd>
<dt><strong><code>previous_meta_file</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>filename for metadata.pkl file saved by previous preprocessing
the thresholds for lifetime are taken from this file</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data(dset=&#39;clath_aux+gak_a7d2&#39;, use_processed=True, save_processed=True,
             processed_file=oj(config.DIR_PROCESSED, &#39;df.pkl&#39;),
             metadata_file=oj(config.DIR_PROCESSED, &#39;metadata.pkl&#39;),
             use_processed_dicts=True,
             compute_dictionary_learning=False,
             outcome_def=&#39;y_consec_thresh&#39;,
             pixel_data: bool=False,
             video_data: bool=True,
             acc_thresh=0.95,
             previous_meta_file: str=None):
    &#39;&#39;&#39;
    Params
    ------
    use_processed: bool, optional
        determines whether to load df from cached pkl
    save_processed: bool, optional
        if not using processed, determines whether to save the df
    use_processed_dicts: bool, optional
        if False, recalculate the dictionary features
    previous_meta_file: str, optional
        filename for metadata.pkl file saved by previous preprocessing
        the thresholds for lifetime are taken from this file
    &#39;&#39;&#39;
    # get things based onn dset
    DSET = config.DSETS[dset]
    LABELS = config.LABELS[dset]

    processed_file = processed_file[:-4] + &#39;_&#39; + dset + &#39;.pkl&#39;
    metadata_file = metadata_file[:-4] + &#39;_&#39; + dset + &#39;.pkl&#39;

    if use_processed and os.path.exists(processed_file):
        return pd.read_pickle(processed_file)
    else:
        print(&#39;loading + preprocessing data...&#39;)
        metadata = {}
        
        
        # load tracks
        print(&#39;\tloading tracks...&#39;)
        df = load_tracking.get_tracks(data_dir=DSET[&#39;data_dir&#39;],
                                      split=DSET, 
                                      pixel_data=pixel_data, 
                                      video_data=video_data,
                                      dset=dset)  # note: different Xs can be different shapes
#         df = df.fillna(df.median()) # this only does anything for the dynamin tracks, where x_pos is sometimes NaN
#         print(&#39;num nans&#39;, df.isna().sum())
        df[&#39;pid&#39;] = np.arange(df.shape[0])  # assign each track a unique id
        df[&#39;valid&#39;] = True  # all tracks start as valid
        
        # set testing tracks to not valid
        if DSET[&#39;test&#39;] is not None:
            df[&#39;valid&#39;][df.cell_num.isin(DSET[&#39;test&#39;])] = False
        metadata[&#39;num_tracks&#39;] = df.valid.sum()
        # print(&#39;training&#39;, df.valid.sum())

        
        
        # preprocess data
        print(&#39;\tpreprocessing data...&#39;)
        df = remove_invalid_tracks(df)  # use catIdx
        # print(&#39;valid&#39;, df.valid.sum())
        df = features.add_basic_features(df)
        df = outcomes.add_outcomes(df, LABELS=LABELS)

        metadata[&#39;num_tracks_valid&#39;] = df.valid.sum()
        metadata[&#39;num_aux_pos_valid&#39;] = df[df.valid][outcome_def].sum()
        metadata[&#39;num_hotspots_valid&#39;] = df[df.valid][&#39;hotspots&#39;].sum()
        df[&#39;valid&#39;][df.hotspots] = False
        df, meta_lifetime = process_tracks_by_lifetime(df, outcome_def=outcome_def,
                                                       plot=False, acc_thresh=acc_thresh,
                                                       previous_meta_file=previous_meta_file)
        df[&#39;valid&#39;][df.short] = False
        df[&#39;valid&#39;][df.long] = False
        metadata.update(meta_lifetime)
        metadata[&#39;num_tracks_hard&#39;] = df[&#39;valid&#39;].sum()
        metadata[&#39;num_aux_pos_hard&#39;] = int(df[df.valid == 1][outcome_def].sum())

        
        # add features
        print(&#39;\tadding features...&#39;)
        df = features.add_dasc_features(df)
        if compute_dictionary_learning:
            df = features.add_dict_features(df, use_processed=use_processed_dicts)
        # df = features.add_smoothed_tracks(df)
        # df = features.add_pcs(df)
        # df = features.add_trend_filtering(df) 
        # df = features.add_binary_features(df, outcome_def=outcome_def)
        if save_processed:
            print(&#39;\tsaving...&#39;)
            pkl.dump(metadata, open(metadata_file, &#39;wb&#39;))
            df.to_pickle(processed_file)
    return df</code></pre>
</details>
</dd>
<dt id="src.data.get_feature_names"><code class="name flex">
<span>def <span class="ident">get_feature_names</span></span>(<span>df)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns features (all of which are scalar)
Removes metadata + time-series columns + outcomes</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_feature_names(df):
    &#39;&#39;&#39;Returns features (all of which are scalar)
    Removes metadata + time-series columns + outcomes
    &#39;&#39;&#39;
    ks = list(df.keys())
    feat_names = [
        k for k in ks
        if not k.startswith(&#39;y&#39;)
           and not k.startswith(&#39;Y&#39;)
           and not k.startswith(&#39;Z&#39;)
           and not k.startswith(&#39;pixel&#39;)
           #         and not k.startswith(&#39;pc_&#39;)
           and not k in [&#39;catIdx&#39;, &#39;cell_num&#39;, &#39;pid&#39;, &#39;valid&#39;,  # metadata
                         &#39;X&#39;, &#39;X_pvals&#39;, &#39;x_pos&#39;, &#39;X_starts&#39;, &#39;X_ends&#39;, &#39;X_extended&#39;,  # curves
                         &#39;short&#39;, &#39;long&#39;, &#39;hotspots&#39;, &#39;sig_idxs&#39;,  # should be weeded out
                         &#39;X_max_around_Y_peak&#39;, &#39;X_max_after_Y_peak&#39;,  # redudant with X_max / fall
                         &#39;X_max_diff&#39;, &#39;X_peak_idx&#39;,  # unlikely to be useful
                         &#39;t&#39;, &#39;x_pos_seq&#39;, &#39;y_pos_seq&#39;,  # curves
                         &#39;X_smooth_spl&#39;, &#39;X_smooth_spl_dx&#39;, &#39;X_smooth_spl_d2x&#39;,  # curves
                         &#39;X_quantiles&#39;,
                         ]
    ]
    return feat_names</code></pre>
</details>
</dd>
<dt id="src.data.load_dfs_for_lstm"><code class="name flex">
<span>def <span class="ident">load_dfs_for_lstm</span></span>(<span>dsets=['clath_aux+gak_new'], splits=['test'], meta=['cell_num', 'Y_sig_mean', 'Y_sig_mean_normalized'], length=40, normalize=True, filter_short=True, padding='end')</span>
</code></dt>
<dd>
<section class="desc"><p>Loads dataframes preprocessed ready for LSTM</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_dfs_for_lstm(dsets=[&#39;clath_aux+gak_new&#39;],
                      splits=[&#39;test&#39;],
                      meta=[&#39;cell_num&#39;, &#39;Y_sig_mean&#39;, &#39;Y_sig_mean_normalized&#39;],
                      length=40,
                      normalize=True,
                      filter_short=True,
                      padding=&#39;end&#39;):
    &#39;&#39;&#39;Loads dataframes preprocessed ready for LSTM
    &#39;&#39;&#39;
    dfs = {}
    for dset in tqdm(dsets):
        for split in splits:
            df = get_data(dset=dset)
            if filter_short:
                df = df[~(df.short | df.long | df.hotspots)]
        #         df = df[df.valid]
                df = df[df.lifetime &gt; 15] # only keep hard tracks
            else:
                df = df[~df.hotspots]
            df = df[df.cell_num.isin(config.DSETS[dset][split])] # exclude held-out test data
            feat_names = [&#39;X_same_length_normalized&#39;] + select_final_feats(get_feature_names(df))

            # downsample tracks
            df[&#39;X_same_length&#39;] = [features.downsample(df.iloc[i][&#39;X&#39;], length, padding=padding)
                                   for i in range(len(df))] # downsampling
            # normalize tracks
            df = features.normalize_track(df, track=&#39;X_same_length&#39;, by_time_point=False)

            # regression response
            df = train_reg.add_sig_mean(df, resp_tracks=[&#39;Y&#39;])     

            # remove extraneous feats
            # df = df[feat_names + meta]
    #         df = df.dropna() 

            # normalize features
            if normalize:
                for feat in feat_names:
                    if &#39;X_same_length&#39; not in feat:
                        df = features.normalize_feature(df, feat)

            dfs[(dset, split)] = deepcopy(df)
    return dfs, feat_names</code></pre>
</details>
</dd>
<dt id="src.data.process_tracks_by_lifetime"><code class="name flex">
<span>def <span class="ident">process_tracks_by_lifetime</span></span>(<span>df, outcome_def, plot=False, acc_thresh=0.95, previous_meta_file=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate accuracy you can get by just predicting max class
as a func of lifetime and return points within proper lifetime (only looks at training cells)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_tracks_by_lifetime(df: pd.DataFrame, outcome_def: str,
                               plot=False, acc_thresh=0.95, previous_meta_file=None):
    &#39;&#39;&#39;Calculate accuracy you can get by just predicting max class 
    as a func of lifetime and return points within proper lifetime (only looks at training cells)
    &#39;&#39;&#39;
    vals = df[df.valid == 1][[&#39;lifetime&#39;, outcome_def]]

    R, C = 1, 3
    lifetimes = np.unique(vals[&#39;lifetime&#39;])

    # cumulative accuracy for different thresholds
    accs_cum_lower = np.array([1 - np.mean(vals[outcome_def][vals[&#39;lifetime&#39;] &lt;= l]) for l in lifetimes])
    accs_cum_higher = np.array([np.mean(vals[outcome_def][vals[&#39;lifetime&#39;] &gt;= l]) for l in lifetimes]).flatten()

    if previous_meta_file is None:
        try:
            idx_thresh = np.nonzero(accs_cum_lower &gt;= acc_thresh)[0][-1]  # last nonzero index
            thresh_lower = lifetimes[idx_thresh]
        except:
            idx_thresh = 0
            thresh_lower = lifetimes[idx_thresh] - 1
        try:
            idx_thresh_2 = np.nonzero(accs_cum_higher &gt;= acc_thresh)[0][0]
            thresh_higher = lifetimes[idx_thresh_2]
        except:
            idx_thresh_2 = lifetimes.size - 1
            thresh_higher = lifetimes[idx_thresh_2] + 1
    else:
        previous_meta = pkl.load(open(previous_meta_file, &#39;rb&#39;))
        thresh_lower = previous_meta[&#39;thresh_short&#39;]
        thresh_higher = previous_meta[&#39;thresh_long&#39;]

    # only df with lifetimes in proper range
    df[&#39;short&#39;] = df[&#39;lifetime&#39;] &lt;= thresh_lower
    df[&#39;long&#39;] = df[&#39;lifetime&#39;] &gt;= thresh_higher
    n = vals.shape[0]
    n_short = np.sum(df[&#39;short&#39;])
    n_long = np.sum(df[&#39;long&#39;])
    acc_short = 1 - np.mean(vals[outcome_def][vals[&#39;lifetime&#39;] &lt;= thresh_lower])
    acc_long = np.mean(vals[outcome_def][vals[&#39;lifetime&#39;] &gt;= thresh_higher])

    metadata = {&#39;num_short&#39;: n_short, &#39;num_long&#39;: n_long, &#39;acc_short&#39;: acc_short,
                &#39;acc_long&#39;: acc_long, &#39;thresh_short&#39;: thresh_lower, &#39;thresh_long&#39;: thresh_higher}

    if plot:
        plt.figure(figsize=(12, 4), dpi=200)
        plt.subplot(R, C, 1)
        outcome = df[outcome_def]
        plt.hist(df[&#39;lifetime&#39;][outcome == 1], label=&#39;aux+&#39;, alpha=1, color=cb, bins=25)
        plt.hist(df[&#39;lifetime&#39;][outcome == 0], label=&#39;aux-&#39;, alpha=0.7, color=cr, bins=25)
        plt.xlabel(&#39;lifetime&#39;)
        plt.ylabel(&#39;count&#39;)
        plt.legend()

        plt.subplot(R, C, 2)
        plt.plot(lifetimes, accs_cum_lower, color=cr)
        #     plt.axvline(thresh_lower)
        plt.axvspan(0, thresh_lower, alpha=0.2, color=cr)
        plt.ylabel(&#39;fraction of negative events&#39;)
        plt.xlabel(f&#39;lifetime &lt;= value\nshaded includes {n_short / n * 100:0.0f}% of pts&#39;)

        plt.subplot(R, C, 3)
        plt.plot(lifetimes, accs_cum_higher, cb)
        plt.axvspan(thresh_higher, max(lifetimes), alpha=0.2, color=cb)
        plt.ylabel(&#39;fraction of positive events&#39;)
        plt.xlabel(f&#39;lifetime &gt;= value\nshaded includes {n_long / n * 100:0.0f}% of pts&#39;)
        plt.tight_layout()

    return df, metadata</code></pre>
</details>
</dd>
<dt id="src.data.remove_invalid_tracks"><code class="name flex">
<span>def <span class="ident">remove_invalid_tracks</span></span>(<span>df, keep=[1, 2])</span>
</code></dt>
<dd>
<section class="desc"><p>Remove certain types of tracks based on cat_idx.
Only keep cat_idx
= 1 and 2
1-4 (non-complex trajectory - no merges and splits)
1 - valid
2 - signal occasionally drops out
3 - cut
- starts / ends
4 - multiple - at the same place (continues throughout)
5-8 (there is merging or splitting)</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_invalid_tracks(df, keep=[1, 2]):
    &#39;&#39;&#39;Remove certain types of tracks based on cat_idx.
    Only keep cat_idx  = 1 and 2
    1-4 (non-complex trajectory - no merges and splits)
        1 - valid
        2 - signal occasionally drops out
        3 - cut  - starts / ends
        4 - multiple - at the same place (continues throughout)
    5-8 (there is merging or splitting)
    &#39;&#39;&#39;
    return df[df.catIdx.isin(keep)]</code></pre>
</details>
</dd>
<dt id="src.data.select_final_feats"><code class="name flex">
<span>def <span class="ident">select_final_feats</span></span>(<span>feat_names, binarize=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select_final_feats(feat_names, binarize=False):
    feat_names = [x for x in feat_names
                  if not x.startswith(&#39;sc_&#39;)  # sparse coding
                  and not x.startswith(&#39;nmf_&#39;) # nmf
                  and not x in [&#39;center_max&#39;, &#39;left_max&#39;, &#39;right_max&#39;, &#39;up_max&#39;, &#39;down_max&#39;,
                                &#39;X_max_around_Y_peak&#39;, &#39;X_max_after_Y_peak&#39;, &#39;X_max_diff_after_Y_peak&#39;]
                  and not x.startswith(&#39;pc_&#39;)
                  and not &#39;extended&#39; in x
                  and not x == &#39;slope_end&#39;
                  and not &#39;_tf_smooth&#39; in x
                  and not &#39;local&#39; in x
                  and not &#39;last&#39; in x
                  and not &#39;video&#39; in x
                  and not x == &#39;X_quantiles&#39;
                  #               and not &#39;X_peak&#39; in x
                  #               and not &#39;slope&#39; in x
                  #               and not x in [&#39;fall_final&#39;, &#39;fall_slope&#39;, &#39;fall_imp&#39;, &#39;fall&#39;]
                  ]

    if binarize:
        feat_names = [x for x in feat_names if &#39;binary&#39; in x]
    else:
        feat_names = [x for x in feat_names if not &#39;binary&#39; in x]
    return feat_names</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.data.get_data" href="#src.data.get_data">get_data</a></code></li>
<li><code><a title="src.data.get_feature_names" href="#src.data.get_feature_names">get_feature_names</a></code></li>
<li><code><a title="src.data.load_dfs_for_lstm" href="#src.data.load_dfs_for_lstm">load_dfs_for_lstm</a></code></li>
<li><code><a title="src.data.process_tracks_by_lifetime" href="#src.data.process_tracks_by_lifetime">process_tracks_by_lifetime</a></code></li>
<li><code><a title="src.data.remove_invalid_tracks" href="#src.data.remove_invalid_tracks">remove_invalid_tracks</a></code></li>
<li><code><a title="src.data.select_final_feats" href="#src.data.select_final_feats">select_final_feats</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>