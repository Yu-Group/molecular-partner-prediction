{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from os.path import join as oj\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import viz\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "# plt.style.use('dark_background')\n",
    "# import full_data_reg\n",
    "import data\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch import NeuralNetRegressor\n",
    "from config import *\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import train_reg\n",
    "from math import floor\n",
    "from copy import deepcopy\n",
    "import config\n",
    "import models\n",
    "import pandas as pd\n",
    "import features\n",
    "from scipy.stats import skew, pearsonr\n",
    "import outcomes\n",
    "import neural_networks\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import lime, shap\n",
    "NUM_EXAMPLES = 100\n",
    "output_dir = '../data/outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently trained only on 'clath_aux+gak_a7d2_new'\n",
    "dsets = ['clath_aux_dynamin']\n",
    "splits = ['test']\n",
    "#feat_names = ['X_same_length_normalized'] + data.select_final_feats(data.get_feature_names(df))\n",
    "              #['mean_total_displacement', 'mean_square_displacement', 'lifetime']\n",
    "meta = ['cell_num', 'Y_sig_mean', 'Y_sig_mean_normalized']\n",
    "dfs, feat_names = data.load_dfs_for_lstm(dsets=dsets, splits=splits, meta=meta)\n",
    "\n",
    "# load model\n",
    "p = 1\n",
    "results = pkl.load(open(config.FINAL_MODEL, 'rb'))\n",
    "dnn = neural_networks.neural_net_sklearn(D_in=40, H=20, p=p-1, arch='lstm')\n",
    "\n",
    "# p = 4\n",
    "# results = pkl.load(open('../models/dnn_full_long_normalized_across_track_4_feats.pkl', 'rb'))\n",
    "# dnn = neural_networks.neural_net_sklearn(D_in=40, H=40, p=p-1, arch='lstm')\n",
    "\n",
    "# p = 18\n",
    "# results = pkl.load(open('../models/clath_aux+gak_a7d2_new_Y_sig_mean_normalized_nn_lstm_100_40.pkl', 'rb'))\n",
    "# dnn = neural_networks.neural_net_sklearn(D_in=40, H=40, p=p-1, arch='lstm')\n",
    "\n",
    "dnn.model.load_state_dict(results['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare for interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dfs[('clath_aux+gak_a7d2', 'train')]\n",
    "# df = dfs[('clath_aux+gak_new', 'test')]\n",
    "df = dfs[('clath_aux_dynamin', 'test')]\n",
    "X = df[feat_names[:p]]\n",
    "y = df['y_consec_thresh']\n",
    "\n",
    "def expand_cols(X):\n",
    "    '''Expand time series into column for each time point\n",
    "    '''\n",
    "    col_names_expanded = [f'clath{i}' for i in range(X['X_same_length_normalized'].iloc[0].size)]\n",
    "    X[col_names_expanded] = pd.DataFrame(X['X_same_length_normalized'].tolist(), index=X.index)\n",
    "    X = X.drop(columns=['X_same_length_normalized'])\n",
    "    return X\n",
    "\n",
    "def shrink_cols(X):\n",
    "    '''Undo time series expansion\n",
    "    '''\n",
    "    # this conversion is only necessary bc the model requires a pd dataframe\n",
    "    # this conversion assumes there are no auxiliary features\n",
    "    if type(X) == np.ndarray:\n",
    "        X = pd.DataFrame(data=X, columns=[f'clath{i}' for i in range(X.shape[1])])\n",
    "    feats_timesteps = [feat for feat in X_expanded.columns if feat.startswith('clath')]\n",
    "    X.insert(0, 'X_same_length_normalized', X[feats_timesteps].values.tolist())\n",
    "    X = X.drop(columns=feats_timesteps)\n",
    "    return X\n",
    "\n",
    "X_expanded = expand_cols(X)\n",
    "\n",
    "def predict_expanded(X_expanded):\n",
    "    return dnn.predict(shrink_cols(X_expanded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 8461 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    }
   ],
   "source": [
    "explainer_lime = lime.lime_tabular.LimeTabularExplainer(training_data=X_expanded.values,\n",
    "                                                   feature_names=X_expanded.columns,\n",
    "                                                   mode='regression')\n",
    "explainer_shap = shap.KernelExplainer(predict_expanded, X_expanded) # link function should be though about\n",
    "\n",
    "fname = oj(output_dir, 'unit_feat_importances_dynamin.pkl')\n",
    "try:\n",
    "    scores = pkl.load(open(fname, 'rb'))\n",
    "except:\n",
    "    scores = {\n",
    "        s: [] for s in ['shap', 'lime']\n",
    "    }\n",
    "    for i in tqdm(range(NUM_EXAMPLES)):\n",
    "        x = X_expanded.iloc[i]\n",
    "        lime_explanation = explainer_lime.explain_instance(x, predict_expanded, num_features=x.size) #, top_labels=1)\n",
    "        lime_values = [l[1] for l in lime_explanation.as_list()]\n",
    "        scores['lime'].append(lime_values)\n",
    "        shap_values = explainer_shap.shap_values(x, nsamples=50)\n",
    "        scores['shap'].append(shap_values)\n",
    "    for k in scores.keys():\n",
    "        scores[k] = np.vstack(scores[k])    \n",
    "\n",
    "    pkl.dump(scores, open(fname, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 40)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores['shap'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bab77e981dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "scores[k][idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-45f0acd73c6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "idxs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "cmap = LinearSegmentedColormap.from_list(\n",
    "        name='orange-blue', \n",
    "        colors=[(222/255, 85/255, 51/255),\n",
    "                'lightgray',\n",
    "                (50/255, 129/255, 168/255)]\n",
    "    )\n",
    "\n",
    "# plot clustermap\n",
    "vabs=0.3\n",
    "for neg in [False, True]:\n",
    "    for k in ['shap', 'lime']:\n",
    "        cm = sns.diverging_palette(22, 220, as_cmap=True, center='light')\n",
    "        idxs = y.values[:NUM_EXAMPLES].astype(bool)\n",
    "        if neg:\n",
    "            idxs = ~idxs\n",
    "        cg = sns.clustermap(scores[k][idxs], col_cluster=False, cmap=cm, center=0,\n",
    "                            norm=matplotlib.colors.Normalize(vmin=-vabs, vmax=vabs))\n",
    "        cg.ax_row_dendrogram.set_visible(False)\n",
    "        hm = cg.ax_heatmap\n",
    "        hm.set_xlabel('Time')\n",
    "        hm.set_ylabel('Tracks')\n",
    "        hm.set_yticks([])\n",
    "        hm.set_xticks([])\n",
    "        s = 'aux-' if neg else 'aux+'\n",
    "        hm.set_title(k.upper() + ' ' + s)\n",
    "        viz.savefig(f'heatmap_{k}_{s}', png=False)\n",
    "        # plt.xticks(np.arange(0, 40, 2), xticklabels=[])\n",
    "#         plt.suptitle(\"A\", y=1.1, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global feat imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_importance(X, model, feat_names):\n",
    "    '''custom bc some features are lists (e.g. the track)'''\n",
    "    acc_orig = np.mean((dnn.predict(X) > 0) == y)\n",
    "    imps = {feat_name: [] for feat_name in feat_names}\n",
    "    reps = 3\n",
    "    for feat_name in tqdm(feat_names):\n",
    "        for r in range(reps):\n",
    "            X_copy = deepcopy(X)\n",
    "            X_copy[feat_name] = np.random.permutation(X_copy[feat_name].values)\n",
    "            acc = np.mean((dnn.predict(X_copy) > 0) == y)\n",
    "            imps[feat_name].append(acc_orig - acc)\n",
    "    return pd.DataFrame.from_dict(imps).mean().sort_values(ascending=False)\n",
    "\n",
    "np.random.seed(13)\n",
    "imps = perm_importance(X, dnn, feat_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
