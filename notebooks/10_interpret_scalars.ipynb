{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from os.path import join as oj\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import viz\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('dark_background')\n",
    "import full_data_reg\n",
    "import data\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch import NeuralNetRegressor\n",
    "from config import *\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import train_reg\n",
    "from math import floor\n",
    "from copy import deepcopy\n",
    "import config\n",
    "import models\n",
    "import pandas as pd\n",
    "import features\n",
    "from scipy.stats import skew, pearsonr\n",
    "import outcomes\n",
    "import neural_networks\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import acd\n",
    "from acd.scores import cd_propagate\n",
    "import lime, shap\n",
    "NUM_EXAMPLES = 300\n",
    "output_dir = '../data/outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currently trained only on 'clath_aux+gak_a7d2_new'\n",
    "dsets = ['clath_aux+gak_new']\n",
    "splits = ['test']\n",
    "#feat_names = ['X_same_length_normalized'] + data.select_final_feats(data.get_feature_names(df))\n",
    "              #['mean_total_displacement', 'mean_square_displacement', 'lifetime']\n",
    "meta = ['cell_num', 'Y_sig_mean', 'Y_sig_mean_normalized']\n",
    "dfs = {}\n",
    "for dset in tqdm(dsets):\n",
    "    for split in splits:\n",
    "        df = data.get_data(dset=dset)\n",
    "        df = df[~(df.short | df.long | df.hotspots)]\n",
    "#         df = df[df.valid]\n",
    "        df = df[df.lifetime > 15] # only keep hard tracks\n",
    "        df = df[df.cell_num.isin(config.DSETS[dset][split])] # exclude held-out test data\n",
    "        feat_names = ['X_same_length_normalized'] + data.select_final_feats(data.get_feature_names(df))\n",
    "\n",
    "        # downsample tracks\n",
    "        length = 40\n",
    "        df['X_same_length'] = [features.downsample(df.iloc[i]['X'], length)\n",
    "                               for i in range(len(df))] # downsampling\n",
    "        # normalize tracks\n",
    "        df = features.normalize_track(df, track='X_same_length', by_time_point=False)\n",
    "\n",
    "        # regression response\n",
    "        df = train_reg.add_sig_mean(df)     \n",
    "\n",
    "        # remove extraneous feats\n",
    "        # df = df[feat_names + meta]\n",
    "#         df = df.dropna() \n",
    "\n",
    "        # normalize features\n",
    "        for feat in feat_names:\n",
    "            if 'X_same_length' not in feat:\n",
    "                df = features.normalize_feature(df, feat)\n",
    "\n",
    "        dfs[(dset, split)] = deepcopy(df)\n",
    "\n",
    "# load model\n",
    "p = 1\n",
    "results = pkl.load(open('../models/dnn_full_long_normalized_across_track_1_feat.pkl', 'rb'))\n",
    "dnn = neural_networks.neural_net_sklearn(D_in=40, H=20, p=p-1, arch='lstm')\n",
    "\n",
    "# p = 4\n",
    "# results = pkl.load(open('../models/dnn_full_long_normalized_across_track_4_feats.pkl', 'rb'))\n",
    "# dnn = neural_networks.neural_net_sklearn(D_in=40, H=40, p=p-1, arch='lstm')\n",
    "\n",
    "# p = 18\n",
    "# results = pkl.load(open('../models/clath_aux+gak_a7d2_new_Y_sig_mean_normalized_nn_lstm_100_40.pkl', 'rb'))\n",
    "# dnn = neural_networks.neural_net_sklearn(D_in=40, H=40, p=p-1, arch='lstm')\n",
    "\n",
    "dnn.model.load_state_dict(results['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare for interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dfs[('clath_aux+gak_a7d2', 'train')]\n",
    "df = dfs[('clath_aux+gak_new', 'test')]\n",
    "X = df[feat_names[:p]]\n",
    "y = df['y_consec_thresh']\n",
    "\n",
    "def expand_cols(X):\n",
    "    '''Expand time series into column for each time point\n",
    "    '''\n",
    "    col_names_expanded = [f'clath{i}' for i in range(X['X_same_length_normalized'].iloc[0].size)]\n",
    "    X[col_names_expanded] = pd.DataFrame(X['X_same_length_normalized'].tolist(), index=X.index)\n",
    "    X = X.drop(columns=['X_same_length_normalized'])\n",
    "    return X\n",
    "\n",
    "def shrink_cols(X):\n",
    "    '''Undo time series expansion\n",
    "    '''\n",
    "    # this conversion is only necessary bc the model requires a pd dataframe\n",
    "    # this conversion assumes there are no auxiliary features\n",
    "    if type(X) == np.ndarray:\n",
    "        X = pd.DataFrame(data=X, columns=[f'clath{i}' for i in range(X.shape[1])])\n",
    "    feats_timesteps = [feat for feat in X_expanded.columns if feat.startswith('clath')]\n",
    "    X.insert(0, 'X_same_length_normalized', X[feats_timesteps].values.tolist())\n",
    "    X = X.drop(columns=feats_timesteps)\n",
    "    return X\n",
    "\n",
    "X_expanded = expand_cols(X)\n",
    "\n",
    "def predict_expanded(X_expanded):\n",
    "    return dnn.predict(shrink_cols(X_expanded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 633 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100%|██████████| 300/300 [37:19<00:00,  7.47s/it]\n"
     ]
    }
   ],
   "source": [
    "explainer_lime = lime.lime_tabular.LimeTabularExplainer(training_data=X_expanded.values,\n",
    "                                                   feature_names=X_expanded.columns,\n",
    "                                                   mode='regression')\n",
    "explainer_shap = shap.KernelExplainer(predict_expanded, X_expanded) # link function should be though about\n",
    "\n",
    "scores = {\n",
    "    s: [] for s in ['shap', 'lime']\n",
    "}\n",
    "for i in tqdm(range(NUM_EXAMPLES)):\n",
    "    x = X_expanded.iloc[i]\n",
    "    lime_explanation = explainer_lime.explain_instance(x, predict_expanded, num_features=x.size) #, top_labels=1)\n",
    "    lime_values = [l[1] for l in lime_explanation.as_list()]\n",
    "    scores['lime'].append(lime_values)\n",
    "    shap_values = explainer_shap.shap_values(x, nsamples=100)\n",
    "    scores['shap'].append(shap_values)\n",
    "for k in scores.keys():\n",
    "    scores[k] = np.vstack(scores[k])    \n",
    "\n",
    "pkl.dump(scores, open(oj(output_dir, 'unit_feat_importances.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clustering large matrix with scipy. Installing `fastcluster` may give better performance.\n"
     ]
    }
   ],
   "source": [
    "# plot clustermap\n",
    "scores = pkl.load(open(oj(output_dir, 'unit_feat_importances.pkl'), 'rb'))\n",
    "for neg in [False, True]:\n",
    "    for k in ['shap', 'lime']:\n",
    "        cm = sns.diverging_palette(22, 220, as_cmap=True, center='dark')\n",
    "        idxs = y[:NUM_EXAMPLES]\n",
    "        if neg:\n",
    "            idxs = ~idxs\n",
    "        cg = sns.clustermap(scores[k], col_cluster=False, cmap=cm)\n",
    "        cg.ax_row_dendrogram.set_visible(False)\n",
    "        hm = cg.ax_heatmap\n",
    "        hm.set_xlabel('Time')\n",
    "        hm.set_ylabel('Tracks')\n",
    "        hm.set_yticks([])\n",
    "        hm.set_xticks([])\n",
    "        hm.set_title(k)\n",
    "        viz.savefig(f'heatmap_{k}_neg={neg}', png=True)\n",
    "        # plt.xticks(np.arange(0, 40, 2), xticklabels=[])\n",
    "        plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot val vs score\n",
    "scores = pkl.load(open(oj(output_dir, 'unit_feat_importances.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global feat imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_importance(X, model, feat_names):\n",
    "    '''custom bc some features are lists (e.g. the track)'''\n",
    "    acc_orig = np.mean((dnn.predict(X) > 0) == y)\n",
    "    imps = {feat_name: [] for feat_name in feat_names}\n",
    "    reps = 3\n",
    "    for feat_name in tqdm(feat_names):\n",
    "        for r in range(reps):\n",
    "            X_copy = deepcopy(X)\n",
    "            X_copy[feat_name] = np.random.permutation(X_copy[feat_name].values)\n",
    "            acc = np.mean((dnn.predict(X_copy) > 0) == y)\n",
    "            imps[feat_name].append(acc_orig - acc)\n",
    "    return pd.DataFrame.from_dict(imps).mean().sort_values(ascending=False)\n",
    "\n",
    "np.random.seed(13)\n",
    "imps = perm_importance(X, dnn, feat_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
