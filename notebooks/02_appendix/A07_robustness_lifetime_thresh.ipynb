{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import dvu\n",
    "import os\n",
    "from os.path import join as oj\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import viz\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "import data\n",
    "from config import *\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import train_reg\n",
    "from copy import deepcopy\n",
    "import config\n",
    "import models\n",
    "import pandas as pd\n",
    "import features\n",
    "import outcomes\n",
    "import neural_networks\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.svm import SVR\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at accs for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = ['clath_aux+gak_a7d2', 'clath_aux+gak', 'clath_aux+gak_a7d2_new', 'clath_aux+gak_new', 'clath_gak', 'clath_aux_dynamin']\n",
    "splits = ['test']\n",
    "#feat_names = [''] + data.select_final_feats(data.get_feature_names(df))\n",
    "              #['mean_total_displacement', 'mean_square_displacement', 'lifetime']\n",
    "length = 40\n",
    "padding = 'end'\n",
    "feat_name = 'X_same_length_extended_normalized' # include buffer X_same_length_normalized\n",
    "outcome = 'Y_sig_mean_normalized'\n",
    "                \n",
    "\n",
    "dfs, feat_names = data.load_dfs_for_lstm(dsets=dsets, \n",
    "                                         splits=splits, \n",
    "                                         lifetime_threshold=-1,\n",
    "                                         length=length,\n",
    "                                         normalize=False,\n",
    "                                         padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_scores(y, preds, y_reg, df, dataset_level_res, cell_level_res):\n",
    "    scorers = {\n",
    "        'balanced_accuracy': metrics.balanced_accuracy_score, 'accuracy': metrics.accuracy_score,\n",
    "        'roc_auc': metrics.roc_auc_score, 'r2': metrics.r2_score,\n",
    "        'corr': scipy.stats.pearsonr\n",
    "    }\n",
    "    for metric in scorers:\n",
    "        if 'accuracy' in metric:\n",
    "            acc = scorers[metric](y, (preds > 0))                   \n",
    "            dataset_level_res[f'{k}_{metric}'].append(acc)\n",
    "        elif metric == 'roc_auc':\n",
    "            try:\n",
    "                dataset_level_res[f'{k}_{metric}'].append(scorers[metric](y, preds))\n",
    "            except:\n",
    "                dataset_level_res[f'{k}_{metric}'].append(np.nan)\n",
    "        elif metric == 'r2':\n",
    "            dataset_level_res[f'{k}_{metric}'].append(scorers[metric](y_reg, preds))\n",
    "        else:\n",
    "            dataset_level_res[f'{k}_{metric}'].append(scorers[metric](y_reg, preds)[0])\n",
    "    '''\n",
    "    for cell in set(df['cell_num']):\n",
    "        cell_idx = np.where(df['cell_num'].values == cell)[0]\n",
    "        y_cell = y[cell_idx]\n",
    "        y_reg_cell = y_reg[cell_idx]\n",
    "        preds_cell = preds[cell_idx]\n",
    "        for metric in scorers:\n",
    "            if 'accuracy' in metric:\n",
    "                acc = scorers[metric](y_cell, (preds_cell > 0))                   \n",
    "                cell_level_res[f'{cell}_{metric}'].append(acc)\n",
    "            elif metric == 'roc_auc':\n",
    "                try:\n",
    "                    cell_level_res[f'{cell}_{metric}'].append(scorers[metric](y_cell, preds_cell))\n",
    "                except:\n",
    "                    cell_level_res[f'{cell}_{metric}'].append(np.nan)\n",
    "            elif metric == 'r2':\n",
    "                cell_level_res[f'{cell}_{metric}'].append(scorers[metric](y_reg_cell, preds_cell))\n",
    "            else:\n",
    "                cell_level_res[f'{cell}_{metric}'].append(scorers[metric](y_reg_cell, preds_cell)[0]) \n",
    "    '''\n",
    "    \n",
    "dataset_level_res = defaultdict(list)\n",
    "cell_level_res = defaultdict(list)\n",
    "LIFETIME_CHUNKS = [(5, 10), (10, 15), (15, 20), (20, 30), (30, 40), (40, 60), (60, 80), (80, 120), (120, 10000)]\n",
    "TRAINING_LIFETIME_THRESHES = [5, 10, 15, 'original', 'baseline']\n",
    "for lifetime_threshold in tqdm(TRAINING_LIFETIME_THRESHES):\n",
    "    \n",
    "    ds = {(k, v): dfs[(k, v)]\n",
    "          for (k, v) in sorted(dfs.keys(), key=lambda x: x[1] + x[0])\n",
    "          if v == 'test'\n",
    "         }\n",
    "    \n",
    "    # load model\n",
    "    if lifetime_threshold == 'original' or lifetime_threshold == 'baseline': # for baseline doesn't matter which model we load, will just alawys predict 0 below\n",
    "        results = pkl.load(open(f'../models/dnn_full_long_normalized_across_track_1_feat_dynamin.pkl', 'rb'))\n",
    "    else:\n",
    "        results = pkl.load(open(f'../models/dnn_fit_extended_lifetimes>{lifetime_threshold}.pkl', 'rb'))\n",
    "    dnn = neural_networks.neural_net_sklearn(D_in=40, H=20, p=0, arch='lstm', track_name=feat_name)\n",
    "    dnn.model.load_state_dict(results['model_state_dict'])\n",
    "\n",
    "    \n",
    "    # evaluate on all data\n",
    "    '''\n",
    "    for i, (k, v) in enumerate(ds.keys()):\n",
    "        df = ds[(k, v)]\n",
    "        X = df[[feat_name]]\n",
    "        y_reg = df['Y_sig_mean_normalized'].values\n",
    "        y = df['y_consec_thresh'].values\n",
    "        preds = dnn.predict(X)\n",
    "        get_all_scores(y, preds, y_reg, df, dataset_level_res, cell_level_res)\n",
    "    '''\n",
    "        \n",
    "        \n",
    "    # lifetime chunks\n",
    "    for chunk in LIFETIME_CHUNKS:\n",
    "        for i, (k, v) in enumerate(ds.keys()):\n",
    "            df = ds[(k, v)]\n",
    "            df = df[(df.lifetime > chunk[0]) & (df.lifetime <= chunk[1])]\n",
    "            X = df[[feat_name]]\n",
    "            # print(chunk, X.shape)\n",
    "            if X.shape[0] > 1:\n",
    "                y_reg = df['Y_sig_mean_normalized'].values\n",
    "                y = df['y_consec_thresh'].values\n",
    "                \n",
    "                if lifetime_threshold == 'baseline':\n",
    "                    preds = np.zeros(y.size)\n",
    "                else:\n",
    "                    preds = dnn.predict(X)\n",
    "                get_all_scores(y, preds, y_reg, df, dataset_level_res, cell_level_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIFETIME_CHUNK_LABS = [str(s) for s in LIFETIME_CHUNKS]\n",
    "R, C = 2, 3\n",
    "plt.figure(figsize=(20, 14), facecolor='w')\n",
    "for i, dset in tqdm(enumerate(dsets)):\n",
    "    ax = plt.subplot(R, C, i + 1)\n",
    "    k = f'{dset}_accuracy'\n",
    "    acc = np.array(dataset_level_res[k]).reshape(len(TRAINING_LIFETIME_THRESHES), len(LIFETIME_CHUNKS))\n",
    "    for i, thresh in enumerate(TRAINING_LIFETIME_THRESHES):\n",
    "        plt.plot(acc[i], label=thresh)\n",
    "    plt.xticks(range(len(LIFETIME_CHUNK_LABS)), labels=LIFETIME_CHUNK_LABS, rotation='60')\n",
    "    dvu.line_legend(adjust_text_labels=True)\n",
    "    plt.title(dset)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Lifetimes')\n",
    "    if i < C:\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "viz.savefig('vary_lifetime_thresh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare different dnn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = ['cell_num', 'Y_sig_mean', 'Y_sig_mean_normalized', 'y_consec_thresh']\n",
    "dfs, feat_names = data.load_dfs_for_lstm(dsets=dsets, splits=splits, meta=meta, normalize=False)\n",
    "# dfs, feat_names = data.load_dfs_for_lstm(dsets=dsets, \n",
    "#                                          splits=splits, \n",
    "#                                          lifetime_threshold=15,\n",
    "#                                          length=length,\n",
    "#                                          normalize=False,\n",
    "#                                          padding=padding)\n",
    "ds = {(k, v): dfs[(k, v)]\n",
    "          for (k, v) in sorted(dfs.keys(), key=lambda x: x[1] + x[0])\n",
    "          if v == 'test'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {l: [] for l in TRAINING_LIFETIME_THRESHES}\n",
    "for i, (k, v) in enumerate(ds.keys()):\n",
    "    df = ds[(k, v)]\n",
    "    for lifetime_threshold in TRAINING_LIFETIME_THRESHES:\n",
    "        # for baseline doesn't matter which model we load, will just alawys predict 0 below\n",
    "        if lifetime_threshold == 'original' or lifetime_threshold == 'baseline':\n",
    "            feat_name = 'X_same_length_normalized'\n",
    "            results = pkl.load(open(f'../models/dnn_full_long_normalized_across_track_1_feat_dynamin.pkl', 'rb'))\n",
    "        else:\n",
    "            feat_name = 'X_same_length_extended_normalized'\n",
    "            results = pkl.load(open(f'../models/dnn_fit_extended_lifetimes>{lifetime_threshold}.pkl', 'rb'))\n",
    "        dnn = neural_networks.neural_net_sklearn(D_in=40, H=20, p=0, arch='lstm', track_name=feat_name)\n",
    "        dnn.model.load_state_dict(results['model_state_dict'])\n",
    "        \n",
    "        X = df[[feat_name]]\n",
    "        y_reg = df['Y_sig_mean_normalized'].values\n",
    "        y = df['y_consec_thresh'].values\n",
    "        if lifetime_threshold == 'baseline':\n",
    "            preds = np.zeros(y.size)\n",
    "        else:\n",
    "            preds = dnn.predict(X)\n",
    "        acc = np.mean((preds > 0) == y)\n",
    "#         print('dset', k, 'model\\t', lifetime_threshold, '\\t', acc.round(3))\n",
    "        accs[lifetime_threshold].append(acc)\n",
    "daccs = pd.DataFrame.from_dict(accs)\n",
    "daccs.index = [k for k,v in ds.keys()]\n",
    "print(daccs.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare different defns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(ds.keys()):\n",
    "    df = ds[(k, v)]\n",
    "    df = outcomes.add_aux_dyn_outcome(df)\n",
    "#     if 'Z' in df.keys():\n",
    "#         plt.hist(df['Z_max'])\n",
    "    print(k, np.mean(df['successful']), np.mean(df['y_consec_thresh']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
