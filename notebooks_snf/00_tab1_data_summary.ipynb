{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from os.path import join as oj\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "import sklearn.calibration\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "# plt.style.use('dark_background')\n",
    "import data\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch import NeuralNetRegressor\n",
    "from config import *\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import train_reg\n",
    "from math import floor\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import config\n",
    "import models\n",
    "import pandas as pd\n",
    "import features\n",
    "from scipy.stats import skew, pearsonr\n",
    "import outcomes\n",
    "import neural_networks\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import interpret\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type <class 'dict'>\n",
      "vps4_snf7 train (7420, 90)\n",
      "vps4_snf7 test (1922, 90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dsets = ['vps4_snf7']\n",
    "splits = ['train', 'test']\n",
    "meta = ['cell_num', 'Y_sig_mean', 'Y_sig_mean_normalized']\n",
    "dfs, feat_names = data.load_dfs_for_lstm(dsets=dsets,\n",
    "                                         splits=splits,\n",
    "                                         filter_hotspots=True,\n",
    "                                         filter_short=True,\n",
    "                                         lifetime_threshold=3,\n",
    "                                         hotspots_threshold=25,\n",
    "                                         meta=meta,\n",
    "                                         normalize=False)\n",
    "\n",
    "\n",
    "print('type', type(dfs))\n",
    "# load model\n",
    "p = 1\n",
    "for k in dfs:\n",
    "    print(*k, dfs[k].shape)\n",
    "df = dfs[list(dfs.keys())[0]]\n",
    "X = df[feat_names[:p]]\n",
    "y = df['y_consec_thresh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**summarize dsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      "          &       &  Total clean tracks &  Difficult tracks &  Difficult valid events &  Short tracks &  Short valid events \\\\\n",
      "Dataset & Partition &                     &                   &                         &               &                     \\\\\n",
      "\\midrule\n",
      "vps4\\_snf7 & test &                2993 &               244 &                     103 &          2749 &                 356 \\\\\n",
      "          & train &                9174 &              1006 &                     411 &          8168 &                1332 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = dfs\n",
    "\n",
    "vals = {} # key is dataset, Partition\n",
    "for k in sorted(ds, key=lambda kv: (kv[0], kv[1])):\n",
    "    d = ds[k]\n",
    "    d.short = d.lifetime <= 15\n",
    "    # print(k, d.lifetime.min())\n",
    "    y = d['y_consec_thresh']\n",
    "    feats = ['Total clean tracks',\n",
    "             'Difficult tracks', 'Difficult valid events', \n",
    "             'Short tracks', 'Short valid events',]\n",
    "    vals[k] = [\n",
    "        d.shape[0],\n",
    "        (~d.short).sum(), y[~d.short].sum(),\n",
    "        d.short.sum(), y[d.short].sum()\n",
    "    ]\n",
    "    vals[k] = [int(x) for x in vals[k]]\n",
    "#     print(feats, vals)\n",
    "df = pd.DataFrame.from_dict(vals).transpose().round(decimals=0)\n",
    "df.columns = feats\n",
    "df.index = df.index.set_names(['Dataset', 'Partition'])\n",
    "print(df.to_latex(index=True, index_names=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
